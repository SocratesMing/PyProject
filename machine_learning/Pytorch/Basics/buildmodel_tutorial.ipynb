{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.608950400Z",
     "start_time": "2023-07-26T12:53:01.233362700Z"
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Build the Neural Network\n",
    "\n",
    "神经网络由对数据执行操作的层/模块组成。\n",
    "[torch.nn](https://pytorch.org/docs/stable/nn.html) 命名空间提供了构建您自己的神经网络所需的所有构建块。\n",
    "PyTorch 中的每个模块都是nn.Module 的子类。神经网络本身就是一个模块，由其他模块（层）组成。\n",
    "这种套结构允许轻松构建和管理复杂的架构。\n",
    "\n",
    "在以下部分中，我们将构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.608950400Z",
     "start_time": "2023-07-26T12:53:01.238361800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取训练设备\n",
    "我们希望能够在 GPU 或 MPS 等硬件加速器（如果可用）上训练我们的模型。\n",
    "让我们检查一下torch.cuda 或torch.backends.mps是否可用，否则我们使用 CPU。\n",
    "[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html)\n",
    "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.608950400Z",
     "start_time": "2023-07-26T12:53:01.243091600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义类\n",
    "\n",
    "我们通过子类化来定义我们的神经网络``nn.Module``，并在``__init__``中初始化神经网络层。\n",
    "每个nn.Module子类都在方法中重写forward方法实现对输入数据的操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.609950200Z",
     "start_time": "2023-07-26T12:53:01.255968500Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建 的一个实例NeuralNetwork，并将其移动到device，并打印其结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.609950200Z",
     "start_time": "2023-07-26T12:53:01.258973500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用该模型，输入数据传递给它。这将执行模型的forward以及一些后台操作[background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)。\n",
    "不要直接调用model.forward()\n",
    "\n",
    "模型会返回一个二维张量，其中 dim=0 对应于每个类别的 10 个原始预测值的每个输出，dim=1 对应于每个输出的各个值。\n",
    "我们通过nn.Softmax实例预测概率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.609950200Z",
     "start_time": "2023-07-26T12:53:01.285579700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型层\n",
    "\n",
    "让我们分解 FashionMNIST 模型中的各个层。为了说明这一点，我们将采用 3 张大小为 28x28 的图像的小批量样本，看看当我们将其传递到网络时会发生什么。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.610950400Z",
     "start_time": "2023-07-26T12:53:01.320771900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "我们初始化nn.Flatten 层 [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "将每个2D 28x28 图像转换为 784 个像素值的连续数组（维持小批量维度（在 dim=0 时））。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.610950400Z",
     "start_time": "2023-07-26T12:53:01.340284700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "线性层 [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "线性层 是一个使用其存储的权重和偏差对输入应用线性变换的模块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.611950900Z",
     "start_time": "2023-07-26T12:53:01.361353700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "Non-linear activations是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后应用以引入非线性，帮助神经网络学习各种现象。\n",
    "在此模型中，我们在线性层之间使用[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)，但还有其他activations可以在模型中引入非线性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.612950500Z",
     "start_time": "2023-07-26T12:53:01.443735600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.3074,  0.0675, -0.0774,  0.0334, -0.0727,  0.2357,  0.6654, -0.1711,\n",
      "          0.5300,  0.1697, -0.2302,  0.1180,  0.4438, -0.1377,  0.0037, -0.1898,\n",
      "         -0.0690,  0.3193, -0.5916, -0.2610],\n",
      "        [ 0.0395,  0.0617,  0.1823,  0.0327,  0.2647, -0.0372,  0.8493, -0.0528,\n",
      "          0.3781,  0.0273, -0.3975,  0.0993,  0.2535, -0.0742, -0.2080,  0.2996,\n",
      "         -0.0400,  0.1853, -0.2708, -0.1649],\n",
      "        [-0.0618, -0.0820,  0.3211,  0.1321,  0.3996,  0.0713,  0.8619, -0.2154,\n",
      "          0.1736,  0.2574, -0.8609, -0.2147,  0.3523, -0.2258, -0.0046, -0.1344,\n",
      "         -0.1148,  0.6483, -0.6095, -0.1567]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.3074, 0.0675, 0.0000, 0.0334, 0.0000, 0.2357, 0.6654, 0.0000, 0.5300,\n",
      "         0.1697, 0.0000, 0.1180, 0.4438, 0.0000, 0.0037, 0.0000, 0.0000, 0.3193,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0395, 0.0617, 0.1823, 0.0327, 0.2647, 0.0000, 0.8493, 0.0000, 0.3781,\n",
      "         0.0273, 0.0000, 0.0993, 0.2535, 0.0000, 0.0000, 0.2996, 0.0000, 0.1853,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3211, 0.1321, 0.3996, 0.0713, 0.8619, 0.0000, 0.1736,\n",
      "         0.2574, 0.0000, 0.0000, 0.3523, 0.0000, 0.0000, 0.0000, 0.0000, 0.6483,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) 是模块的有序容器。\n",
    "数据按照定义的相同顺序传递通过所有模块。您可以使用顺序容器来组合一个快速网络，例如seq_modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.612950500Z",
     "start_time": "2023-07-26T12:53:01.467952900Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values\n",
    "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
    "which the values must sum to 1.\n",
    "\n",
    "神经网络的最后一个线性层返回logits -原始值在区间[-infty, infty}，被传递到 nn.Softmax模块。\n",
    "Logits 缩放为值 [0, 1]，表示模型对每个类别的预测概率。\n",
    "dim参数指示维度，沿该维度值的总和必须为 1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.612950500Z",
     "start_time": "2023-07-26T12:53:01.491704600Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数\n",
    "\n",
    "神经网络内的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。\n",
    "子类nn.Module会自动跟踪模型对象中定义的所有字段，并使所有参数都可以使用模型parameters()或named_parameters()方法进行访问。\n",
    "\n",
    "在此示例中，我们迭代每个参数，并打印其大小及其值的预览。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T12:53:01.626952Z",
     "start_time": "2023-07-26T12:53:01.518963900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0048,  0.0315, -0.0130,  ..., -0.0289, -0.0353,  0.0141],\n",
      "        [ 0.0242, -0.0066, -0.0319,  ...,  0.0299, -0.0068,  0.0126]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0289,  0.0098], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0077,  0.0353, -0.0248,  ...,  0.0126,  0.0215, -0.0017],\n",
      "        [ 0.0229, -0.0149, -0.0011,  ...,  0.0330,  0.0156,  0.0096]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0350, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0015,  0.0226,  0.0369,  ...,  0.0172, -0.0233, -0.0382],\n",
      "        [ 0.0107,  0.0198, -0.0136,  ...,  0.0271,  0.0436,  0.0438]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0167, -0.0140], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
