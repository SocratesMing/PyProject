{
 "cells": [
  {
   "cell_type": "code",
   "id": "ca63463b28836463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:26:03.643600Z",
     "start_time": "2025-04-24T08:25:59.031244Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1. 数据预处理\n",
    "# ======================\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, look_back=60, forecast_horizon=1):\n",
    "        self.look_back = look_back\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))  #将数据线性缩放到 0 到 1 的区间内\n",
    "\n",
    "        # 数据归一化\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "\n",
    "        # 创建滑动窗口数据集\n",
    "        X, y = [], []\n",
    "        for i in range(len(scaled_data) - look_back - forecast_horizon + 1):\n",
    "            X.append(scaled_data[i:i + look_back, :])\n",
    "            y.append(scaled_data[i + look_back + forecast_horizon - 1, 3])  # 预测Close价格\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('./data/USDCNHSP.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# 特征工程\n",
    "df['ohlc_range'] = df['high'] - df['low']\n",
    "df['close_change'] = df['close'] - df['close'].shift(1)\n",
    "print(df.head())\n",
    "features = df[['open', 'high', 'low', 'close', 'ohlc_range', 'close_change']].values\n",
    "\n",
    "# 划分数据集\n",
    "train_size = int(len(features) * 0.8)\n",
    "train_data = features[:train_size]\n",
    "test_data = features[train_size:]\n",
    "\n",
    "# 创建数据集对象\n",
    "look_back = 60\n",
    "dataset = TimeSeriesDataset(train_data, look_back=look_back)\n",
    "test_dataset = TimeSeriesDataset(test_data, look_back=look_back)\n",
    "\n",
    "# 数据加载器\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       symbol     open     high      low    close  ohlc_range  \\\n",
      "date                                                                            \n",
      "2012-12-31 18:00:00  USDCNHSP  6.22118  6.22153  6.22113  6.22117     0.00040   \n",
      "2012-12-31 19:00:00  USDCNHSP  6.22112  6.22141  6.22093  6.22137     0.00048   \n",
      "2012-12-31 20:00:00  USDCNHSP  6.22098  6.22137  6.22043  6.22137     0.00094   \n",
      "2012-12-31 21:00:00  USDCNHSP  6.22062  6.22142  6.22062  6.22141     0.00080   \n",
      "2012-12-31 22:00:00  USDCNHSP  6.22150  6.22182  6.22071  6.22126     0.00111   \n",
      "\n",
      "                     close_change  \n",
      "date                               \n",
      "2012-12-31 18:00:00           NaN  \n",
      "2012-12-31 19:00:00       0.00020  \n",
      "2012-12-31 20:00:00       0.00000  \n",
      "2012-12-31 21:00:00       0.00004  \n",
      "2012-12-31 22:00:00      -0.00015  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:26:15.728139Z",
     "start_time": "2025-04-24T08:26:15.714086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ======================\n",
    "# 2. TCN模型定义\n",
    "# ======================\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_layers=4, kernel_size=3, dilation_factors=[1, 2, 4, 8]):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_channels:\n",
    "        :param num_layers:\n",
    "        :param kernel_size:\n",
    "        :param dilation_factors: 膨胀因子\n",
    "        \"\"\"\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_channels = input_channels\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_factors[i] if i < len(dilation_factors) else 1\n",
    "            padding = (kernel_size - 1) * dilation  #保持时序因果性\n",
    "\n",
    "            # 因果卷积层  + ReLU\n",
    "            conv = nn.Conv1d(num_channels, num_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "            layers += [conv, nn.ReLU()]\n",
    "\n",
    "            # 残差连接\n",
    "            if num_channels != input_channels:\n",
    "                layers += [nn.Conv1d(input_channels, num_channels, 1)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.downsample = nn.Conv1d(input_channels, num_channels, 1) if num_channels != input_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return nn.ReLU()(out + x)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_dim=6, output_dim=1, num_channels=[64, 64, 64, 64]):\n",
    "        super(TCN, self).__init__()\n",
    "        self.encoder = TemporalConvNet(input_dim, num_layers=4, kernel_size=3)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入形状: (batch, seq_len, features) → (batch, features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=2)  # 全局平均池化\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "print(\"模型定义完毕\")"
   ],
   "id": "8190902ce2b6467a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型定义完毕\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:26:57.193139Z",
     "start_time": "2025-04-24T08:26:51.330606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TCN(input_dim=6).to(device)\n",
    "\n",
    "# ======================\n",
    "# 3. 训练流程\n",
    "# ======================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  #学习率为\n",
    "best_loss = float('inf')\n",
    "\n",
    "# 早停机制\n",
    "early_stop = {\n",
    "    'patience': 10,\n",
    "    'counter': 0,\n",
    "    'best_loss': float('inf'),\n",
    "    'wait': 0\n",
    "}\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    for inputs, targets in loop:\n",
    "        # print(inputs,targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计损失\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # 早停判断\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        early_stop['wait'] = 0\n",
    "    else:\n",
    "        early_stop['wait'] += 1\n",
    "        if early_stop['wait'] >= early_stop['patience']:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "print(f'Best Validation Loss: {best_loss:.4f}')\n",
    "\n",
    "# ======================\n",
    "# 4. 预测与可视化\n",
    "# ======================\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 预测函数\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# 反归一化\n",
    "test_loader_full = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets in tqdm(test_loader_full):\n",
    "    y_true.extend(targets.cpu().numpy())\n",
    "    y_pred.extend(predict(model, DataLoader([inputs], batch_size=1))[0])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# 反归一化\n",
    "y_true = scaler.inverse_transform(np.array(y_true)[:, 3].reshape(-1, 1))\n",
    "y_pred = scaler.inverse_transform(np.array(y_pred)[:, 3].reshape(-1, 1))\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(y_true, label='Actual Price')\n",
    "plt.plot(y_pred, label='TCN Prediction')\n",
    "plt.title('Stock Close Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/226 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (90) must match the size of tensor b (60) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     30\u001B[39m inputs, targets = inputs.to(device), targets.to(device)\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m loss = criterion(outputs, targets)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Code\\Python\\PyProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Code\\Python\\PyProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mTCN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     46\u001B[39m     \u001B[38;5;66;03m# 输入形状: (batch, seq_len, features) → (batch, features, seq_len)\u001B[39;00m\n\u001B[32m     47\u001B[39m     x = x.transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     49\u001B[39m     x = x.mean(dim=\u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# 全局平均池化\u001B[39;00m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fc(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Code\\Python\\PyProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Code\\Python\\PyProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mTemporalConvNet.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.downsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     36\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.downsample(x)\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m nn.ReLU()(\u001B[43mout\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m)\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (90) must match the size of tensor b (60) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:26:03.862844700Z",
     "start_time": "2025-04-24T08:22:38.383123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "print(output.backward())"
   ],
   "id": "f887a65c842577b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e5fad36eacd95f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
